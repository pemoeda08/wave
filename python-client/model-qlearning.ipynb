{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import ActionTuple, UnityEnvironment, BaseEnv\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAVE environment created.\n"
     ]
    }
   ],
   "source": [
    "channel = EngineConfigurationChannel()\n",
    "\n",
    "env = UnityEnvironment(file_name='./Wave', seed=1, side_channels=[channel])\n",
    "channel.set_configuration_parameters(time_scale = 20)\n",
    "print(\"WAVE environment created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = 34 + 2\n",
    "l2 = 150\n",
    "l3 = 150\n",
    "l4 = 2\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "  torch.nn.Linear(l1, l2),\n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(l2, l3),\n",
    "  torch.nn.ReLU(),\n",
    "  torch.nn.Linear(l3, l4),\n",
    ")\n",
    "\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(pred, target):\n",
    "    return torch.mean(0.5 * (pred - target) ** 2)\n",
    "\n",
    "def preprocess_input(inp):\n",
    "    return np.append(inp.obs[1], inp.obs[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, total reward: 36.1673583984375, timestep: 89, epsilon: 1\n",
      "EPOCH: 1, total reward: 0.0, timestep: 2, epsilon: 0.998\n",
      "EPOCH: 2, total reward: 2.881955146789551, timestep: 28, epsilon: 0.996\n",
      "EPOCH: 3, total reward: 28.30318832397461, timestep: 86, epsilon: 0.994\n",
      "EPOCH: 4, total reward: 2.8724164962768555, timestep: 27, epsilon: 0.992\n",
      "EPOCH: 5, total reward: 11.162006378173828, timestep: 39, epsilon: 0.99\n",
      "EPOCH: 6, total reward: 2.8469905853271484, timestep: 26, epsilon: 0.988\n",
      "EPOCH: 7, total reward: 2.9827675819396973, timestep: 26, epsilon: 0.986\n",
      "EPOCH: 8, total reward: 52.84823226928711, timestep: 116, epsilon: 0.984\n",
      "EPOCH: 9, total reward: 2.824700355529785, timestep: 26, epsilon: 0.982\n",
      "EPOCH: 10, total reward: 2.9046945571899414, timestep: 29, epsilon: 0.98\n",
      "EPOCH: 11, total reward: 11.159300804138184, timestep: 49, epsilon: 0.978\n",
      "EPOCH: 12, total reward: 36.16903305053711, timestep: 90, epsilon: 0.976\n",
      "EPOCH: 13, total reward: 11.18298053741455, timestep: 51, epsilon: 0.974\n",
      "EPOCH: 14, total reward: 19.52590560913086, timestep: 62, epsilon: 0.972\n",
      "EPOCH: 15, total reward: 86.72660827636719, timestep: 190, epsilon: 0.97\n",
      "EPOCH: 16, total reward: 3.4856367111206055, timestep: 26, epsilon: 0.968\n",
      "EPOCH: 17, total reward: 0.0, timestep: 1, epsilon: 0.966\n",
      "EPOCH: 18, total reward: 11.156984329223633, timestep: 53, epsilon: 0.964\n",
      "EPOCH: 19, total reward: 28.459163665771484, timestep: 75, epsilon: 0.962\n",
      "EPOCH: 20, total reward: 11.471395492553711, timestep: 51, epsilon: 0.96\n",
      "EPOCH: 21, total reward: 2.86041259765625, timestep: 28, epsilon: 0.958\n",
      "EPOCH: 22, total reward: 19.616870880126953, timestep: 62, epsilon: 0.956\n",
      "EPOCH: 23, total reward: 19.531780242919922, timestep: 67, epsilon: 0.954\n",
      "EPOCH: 24, total reward: 2.879251480102539, timestep: 36, epsilon: 0.952\n",
      "EPOCH: 25, total reward: 2.912008285522461, timestep: 29, epsilon: 0.95\n",
      "EPOCH: 26, total reward: 44.843284606933594, timestep: 111, epsilon: 0.948\n",
      "EPOCH: 27, total reward: 44.690643310546875, timestep: 127, epsilon: 0.946\n",
      "EPOCH: 28, total reward: 11.173065185546875, timestep: 39, epsilon: 0.944\n",
      "EPOCH: 29, total reward: 2.922757148742676, timestep: 28, epsilon: 0.942\n",
      "EPOCH: 30, total reward: 28.012910842895508, timestep: 74, epsilon: 0.94\n",
      "EPOCH: 31, total reward: 11.232636451721191, timestep: 42, epsilon: 0.938\n",
      "EPOCH: 32, total reward: 2.8758363723754883, timestep: 38, epsilon: 0.9359999999999999\n",
      "EPOCH: 33, total reward: 2.9171409606933594, timestep: 24, epsilon: 0.9339999999999999\n",
      "EPOCH: 34, total reward: 20.00415802001953, timestep: 64, epsilon: 0.9319999999999999\n",
      "EPOCH: 35, total reward: 19.524715423583984, timestep: 52, epsilon: 0.9299999999999999\n",
      "EPOCH: 36, total reward: 19.833499908447266, timestep: 51, epsilon: 0.9279999999999999\n",
      "EPOCH: 37, total reward: 0.0, timestep: 1, epsilon: 0.9259999999999999\n",
      "EPOCH: 38, total reward: 2.8432188034057617, timestep: 28, epsilon: 0.9239999999999999\n",
      "EPOCH: 39, total reward: 11.147528648376465, timestep: 53, epsilon: 0.9219999999999999\n",
      "EPOCH: 40, total reward: 0.0, timestep: 2, epsilon: 0.9199999999999999\n",
      "EPOCH: 41, total reward: 2.936187744140625, timestep: 23, epsilon: 0.9179999999999999\n",
      "EPOCH: 42, total reward: 0.0, timestep: 10, epsilon: 0.9159999999999999\n",
      "EPOCH: 43, total reward: 2.8396196365356445, timestep: 29, epsilon: 0.9139999999999999\n",
      "EPOCH: 44, total reward: 0.0, timestep: 1, epsilon: 0.9119999999999999\n",
      "EPOCH: 45, total reward: 2.912886619567871, timestep: 27, epsilon: 0.9099999999999999\n",
      "EPOCH: 46, total reward: 0.0, timestep: 4, epsilon: 0.9079999999999999\n",
      "EPOCH: 47, total reward: 53.17098617553711, timestep: 110, epsilon: 0.9059999999999999\n",
      "EPOCH: 48, total reward: 19.582855224609375, timestep: 64, epsilon: 0.9039999999999999\n",
      "EPOCH: 49, total reward: 3.7570104598999023, timestep: 26, epsilon: 0.9019999999999999\n",
      "EPOCH: 50, total reward: 36.741722106933594, timestep: 89, epsilon: 0.8999999999999999\n",
      "EPOCH: 51, total reward: 2.8153936862945557, timestep: 27, epsilon: 0.8979999999999999\n",
      "EPOCH: 52, total reward: 2.841811180114746, timestep: 29, epsilon: 0.8959999999999999\n",
      "EPOCH: 53, total reward: 11.247400283813477, timestep: 50, epsilon: 0.8939999999999999\n",
      "EPOCH: 54, total reward: 11.178857803344727, timestep: 50, epsilon: 0.8919999999999999\n",
      "EPOCH: 55, total reward: 2.896371603012085, timestep: 26, epsilon: 0.8899999999999999\n",
      "EPOCH: 56, total reward: 11.845376968383789, timestep: 51, epsilon: 0.8879999999999999\n",
      "EPOCH: 57, total reward: 52.959835052490234, timestep: 128, epsilon: 0.8859999999999999\n",
      "EPOCH: 58, total reward: 11.221619606018066, timestep: 51, epsilon: 0.8839999999999999\n",
      "EPOCH: 59, total reward: 11.694581985473633, timestep: 38, epsilon: 0.8819999999999999\n",
      "EPOCH: 60, total reward: 2.8976755142211914, timestep: 29, epsilon: 0.8799999999999999\n",
      "EPOCH: 61, total reward: 2.84714412689209, timestep: 27, epsilon: 0.8779999999999999\n",
      "EPOCH: 62, total reward: 44.4891242980957, timestep: 102, epsilon: 0.8759999999999999\n",
      "EPOCH: 63, total reward: 20.338228225708008, timestep: 60, epsilon: 0.8739999999999999\n",
      "EPOCH: 64, total reward: 2.883251190185547, timestep: 29, epsilon: 0.8719999999999999\n",
      "EPOCH: 65, total reward: 36.621551513671875, timestep: 112, epsilon: 0.8699999999999999\n",
      "EPOCH: 66, total reward: 2.8449935913085938, timestep: 27, epsilon: 0.8679999999999999\n",
      "EPOCH: 67, total reward: 3.339296340942383, timestep: 26, epsilon: 0.8659999999999999\n",
      "EPOCH: 68, total reward: 28.039302825927734, timestep: 77, epsilon: 0.8639999999999999\n",
      "EPOCH: 69, total reward: 61.769290924072266, timestep: 127, epsilon: 0.8619999999999999\n",
      "EPOCH: 70, total reward: 2.828815460205078, timestep: 29, epsilon: 0.8599999999999999\n",
      "EPOCH: 71, total reward: 3.1677141189575195, timestep: 24, epsilon: 0.8579999999999999\n",
      "EPOCH: 72, total reward: 2.8902196884155273, timestep: 28, epsilon: 0.8559999999999999\n",
      "EPOCH: 73, total reward: 2.9349474906921387, timestep: 26, epsilon: 0.8539999999999999\n",
      "EPOCH: 74, total reward: 36.2462272644043, timestep: 76, epsilon: 0.8519999999999999\n",
      "EPOCH: 75, total reward: 2.882561683654785, timestep: 29, epsilon: 0.8499999999999999\n",
      "EPOCH: 76, total reward: 2.818349838256836, timestep: 38, epsilon: 0.8479999999999999\n",
      "EPOCH: 77, total reward: 61.74860382080078, timestep: 125, epsilon: 0.8459999999999999\n",
      "EPOCH: 78, total reward: 2.850475311279297, timestep: 28, epsilon: 0.8439999999999999\n",
      "EPOCH: 79, total reward: 28.281246185302734, timestep: 75, epsilon: 0.8419999999999999\n",
      "EPOCH: 80, total reward: 11.265237808227539, timestep: 39, epsilon: 0.8399999999999999\n",
      "EPOCH: 81, total reward: 2.809518814086914, timestep: 27, epsilon: 0.8379999999999999\n",
      "EPOCH: 82, total reward: 11.146598815917969, timestep: 40, epsilon: 0.8359999999999999\n",
      "EPOCH: 83, total reward: 52.896114349365234, timestep: 114, epsilon: 0.8339999999999999\n",
      "EPOCH: 84, total reward: 61.28068542480469, timestep: 126, epsilon: 0.8319999999999999\n",
      "EPOCH: 85, total reward: 2.8473949432373047, timestep: 30, epsilon: 0.8299999999999998\n",
      "EPOCH: 86, total reward: 3.162555694580078, timestep: 22, epsilon: 0.8279999999999998\n",
      "EPOCH: 87, total reward: 2.864351272583008, timestep: 28, epsilon: 0.8259999999999998\n",
      "EPOCH: 88, total reward: 2.858734130859375, timestep: 27, epsilon: 0.8239999999999998\n",
      "EPOCH: 89, total reward: 2.899961471557617, timestep: 27, epsilon: 0.8219999999999998\n",
      "EPOCH: 90, total reward: 2.899221420288086, timestep: 26, epsilon: 0.8199999999999998\n",
      "EPOCH: 91, total reward: 11.41253662109375, timestep: 48, epsilon: 0.8179999999999998\n",
      "EPOCH: 92, total reward: 61.23779296875, timestep: 142, epsilon: 0.8159999999999998\n",
      "EPOCH: 93, total reward: 2.831061363220215, timestep: 27, epsilon: 0.8139999999999998\n",
      "EPOCH: 94, total reward: 2.944582939147949, timestep: 24, epsilon: 0.8119999999999998\n",
      "EPOCH: 95, total reward: 44.582828521728516, timestep: 101, epsilon: 0.8099999999999998\n",
      "EPOCH: 96, total reward: 2.835966110229492, timestep: 29, epsilon: 0.8079999999999998\n",
      "EPOCH: 97, total reward: 27.80867576599121, timestep: 75, epsilon: 0.8059999999999998\n",
      "EPOCH: 98, total reward: 2.9248085021972656, timestep: 27, epsilon: 0.8039999999999998\n",
      "EPOCH: 99, total reward: 36.91062545776367, timestep: 87, epsilon: 0.8019999999999998\n",
      "EPOCH: 100, total reward: 28.586978912353516, timestep: 64, epsilon: 0.7999999999999998\n",
      "EPOCH: 101, total reward: 2.835285186767578, timestep: 27, epsilon: 0.7979999999999998\n",
      "EPOCH: 102, total reward: 61.205299377441406, timestep: 127, epsilon: 0.7959999999999998\n",
      "EPOCH: 103, total reward: 3.3793325424194336, timestep: 25, epsilon: 0.7939999999999998\n",
      "EPOCH: 104, total reward: 2.857534408569336, timestep: 27, epsilon: 0.7919999999999998\n",
      "EPOCH: 105, total reward: 0.0, timestep: 2, epsilon: 0.7899999999999998\n",
      "EPOCH: 106, total reward: 2.867879867553711, timestep: 28, epsilon: 0.7879999999999998\n",
      "EPOCH: 107, total reward: 36.198856353759766, timestep: 87, epsilon: 0.7859999999999998\n",
      "EPOCH: 108, total reward: 19.761981964111328, timestep: 51, epsilon: 0.7839999999999998\n",
      "EPOCH: 109, total reward: 2.8206262588500977, timestep: 29, epsilon: 0.7819999999999998\n",
      "EPOCH: 110, total reward: 194.56460571289062, timestep: 316, epsilon: 0.7799999999999998\n",
      "EPOCH: 111, total reward: 2.9227418899536133, timestep: 24, epsilon: 0.7779999999999998\n",
      "EPOCH: 112, total reward: 11.90544605255127, timestep: 38, epsilon: 0.7759999999999998\n",
      "EPOCH: 113, total reward: 219.55247497558594, timestep: 343, epsilon: 0.7739999999999998\n",
      "EPOCH: 114, total reward: 61.37373733520508, timestep: 124, epsilon: 0.7719999999999998\n",
      "EPOCH: 115, total reward: 2.867377281188965, timestep: 28, epsilon: 0.7699999999999998\n",
      "EPOCH: 116, total reward: 186.36740112304688, timestep: 314, epsilon: 0.7679999999999998\n",
      "EPOCH: 117, total reward: 61.27747344970703, timestep: 105, epsilon: 0.7659999999999998\n",
      "EPOCH: 118, total reward: 3.3592700958251953, timestep: 23, epsilon: 0.7639999999999998\n",
      "EPOCH: 119, total reward: 27.84023666381836, timestep: 78, epsilon: 0.7619999999999998\n",
      "EPOCH: 120, total reward: 2.8673763275146484, timestep: 27, epsilon: 0.7599999999999998\n",
      "EPOCH: 121, total reward: 2.9224014282226562, timestep: 27, epsilon: 0.7579999999999998\n",
      "EPOCH: 122, total reward: 2.92948055267334, timestep: 24, epsilon: 0.7559999999999998\n",
      "EPOCH: 123, total reward: 2.8911638259887695, timestep: 28, epsilon: 0.7539999999999998\n",
      "EPOCH: 124, total reward: 2.871138572692871, timestep: 25, epsilon: 0.7519999999999998\n",
      "EPOCH: 125, total reward: 2.815279960632324, timestep: 26, epsilon: 0.7499999999999998\n",
      "EPOCH: 126, total reward: 11.222268104553223, timestep: 30, epsilon: 0.7479999999999998\n",
      "EPOCH: 127, total reward: 2.820219039916992, timestep: 23, epsilon: 0.7459999999999998\n",
      "EPOCH: 128, total reward: 36.38547134399414, timestep: 77, epsilon: 0.7439999999999998\n",
      "EPOCH: 129, total reward: 3.763977527618408, timestep: 26, epsilon: 0.7419999999999998\n",
      "EPOCH: 130, total reward: 2.858367919921875, timestep: 27, epsilon: 0.7399999999999998\n",
      "EPOCH: 131, total reward: 86.24757385253906, timestep: 143, epsilon: 0.7379999999999998\n",
      "EPOCH: 132, total reward: 103.5538330078125, timestep: 186, epsilon: 0.7359999999999998\n",
      "EPOCH: 133, total reward: 19.579490661621094, timestep: 52, epsilon: 0.7339999999999998\n",
      "EPOCH: 134, total reward: 61.24401092529297, timestep: 104, epsilon: 0.7319999999999998\n",
      "EPOCH: 135, total reward: 11.191933631896973, timestep: 39, epsilon: 0.7299999999999998\n",
      "EPOCH: 136, total reward: 19.525390625, timestep: 51, epsilon: 0.7279999999999998\n",
      "EPOCH: 137, total reward: 2.859015464782715, timestep: 24, epsilon: 0.7259999999999998\n",
      "EPOCH: 138, total reward: 2.9532699584960938, timestep: 27, epsilon: 0.7239999999999998\n",
      "EPOCH: 139, total reward: 3.2965571880340576, timestep: 26, epsilon: 0.7219999999999998\n",
      "EPOCH: 140, total reward: 2.9244747161865234, timestep: 28, epsilon: 0.7199999999999998\n",
      "EPOCH: 141, total reward: 69.50069427490234, timestep: 116, epsilon: 0.7179999999999997\n",
      "EPOCH: 142, total reward: 0.0, timestep: 2, epsilon: 0.7159999999999997\n",
      "EPOCH: 143, total reward: 3.231905937194824, timestep: 23, epsilon: 0.7139999999999997\n",
      "EPOCH: 144, total reward: 11.177925109863281, timestep: 39, epsilon: 0.7119999999999997\n",
      "EPOCH: 145, total reward: 36.1430778503418, timestep: 79, epsilon: 0.7099999999999997\n",
      "EPOCH: 146, total reward: 44.55781936645508, timestep: 90, epsilon: 0.7079999999999997\n",
      "EPOCH: 147, total reward: 28.652496337890625, timestep: 73, epsilon: 0.7059999999999997\n",
      "EPOCH: 148, total reward: 3.474379539489746, timestep: 26, epsilon: 0.7039999999999997\n",
      "EPOCH: 149, total reward: 36.22044372558594, timestep: 89, epsilon: 0.7019999999999997\n",
      "EPOCH: 150, total reward: 2.8276023864746094, timestep: 29, epsilon: 0.6999999999999997\n",
      "EPOCH: 151, total reward: 0.0, timestep: 2, epsilon: 0.6979999999999997\n",
      "EPOCH: 152, total reward: 2.8447265625, timestep: 25, epsilon: 0.6959999999999997\n",
      "EPOCH: 153, total reward: 2.8994951248168945, timestep: 29, epsilon: 0.6939999999999997\n",
      "EPOCH: 154, total reward: 94.55099487304688, timestep: 165, epsilon: 0.6919999999999997\n",
      "EPOCH: 155, total reward: 44.50069808959961, timestep: 88, epsilon: 0.6899999999999997\n",
      "EPOCH: 156, total reward: 86.25103759765625, timestep: 151, epsilon: 0.6879999999999997\n",
      "EPOCH: 157, total reward: 86.23861694335938, timestep: 140, epsilon: 0.6859999999999997\n",
      "EPOCH: 158, total reward: 2.893827438354492, timestep: 25, epsilon: 0.6839999999999997\n",
      "EPOCH: 159, total reward: 2.896566390991211, timestep: 26, epsilon: 0.6819999999999997\n",
      "EPOCH: 160, total reward: 61.18675994873047, timestep: 104, epsilon: 0.6799999999999997\n",
      "EPOCH: 161, total reward: 2.844289779663086, timestep: 25, epsilon: 0.6779999999999997\n",
      "EPOCH: 162, total reward: 2.921706438064575, timestep: 25, epsilon: 0.6759999999999997\n",
      "EPOCH: 163, total reward: 119.51750183105469, timestep: 205, epsilon: 0.6739999999999997\n",
      "EPOCH: 164, total reward: 2.853069305419922, timestep: 24, epsilon: 0.6719999999999997\n",
      "EPOCH: 165, total reward: 3.5343260765075684, timestep: 25, epsilon: 0.6699999999999997\n",
      "EPOCH: 166, total reward: 2.9253597259521484, timestep: 27, epsilon: 0.6679999999999997\n",
      "EPOCH: 167, total reward: 2.8760595321655273, timestep: 26, epsilon: 0.6659999999999997\n",
      "EPOCH: 168, total reward: 44.569244384765625, timestep: 79, epsilon: 0.6639999999999997\n",
      "EPOCH: 169, total reward: 27.900318145751953, timestep: 65, epsilon: 0.6619999999999997\n",
      "EPOCH: 170, total reward: 3.529971122741699, timestep: 23, epsilon: 0.6599999999999997\n",
      "EPOCH: 171, total reward: 27.836214065551758, timestep: 54, epsilon: 0.6579999999999997\n",
      "EPOCH: 172, total reward: 2.9659128189086914, timestep: 24, epsilon: 0.6559999999999997\n",
      "EPOCH: 173, total reward: 27.896480560302734, timestep: 54, epsilon: 0.6539999999999997\n",
      "EPOCH: 174, total reward: 3.172574043273926, timestep: 23, epsilon: 0.6519999999999997\n",
      "EPOCH: 175, total reward: 2.8328418731689453, timestep: 27, epsilon: 0.6499999999999997\n",
      "EPOCH: 176, total reward: 2.836390495300293, timestep: 29, epsilon: 0.6479999999999997\n",
      "EPOCH: 177, total reward: 27.905214309692383, timestep: 52, epsilon: 0.6459999999999997\n",
      "EPOCH: 178, total reward: 2.9028525352478027, timestep: 26, epsilon: 0.6439999999999997\n",
      "EPOCH: 179, total reward: 77.88154602050781, timestep: 138, epsilon: 0.6419999999999997\n",
      "EPOCH: 180, total reward: 2.856865167617798, timestep: 25, epsilon: 0.6399999999999997\n",
      "EPOCH: 181, total reward: 11.270829200744629, timestep: 39, epsilon: 0.6379999999999997\n",
      "EPOCH: 182, total reward: 61.275672912597656, timestep: 104, epsilon: 0.6359999999999997\n",
      "EPOCH: 183, total reward: 2.8844614028930664, timestep: 26, epsilon: 0.6339999999999997\n",
      "EPOCH: 184, total reward: 3.0326967239379883, timestep: 24, epsilon: 0.6319999999999997\n",
      "EPOCH: 185, total reward: 69.5632095336914, timestep: 118, epsilon: 0.6299999999999997\n",
      "EPOCH: 186, total reward: 36.217384338378906, timestep: 75, epsilon: 0.6279999999999997\n",
      "EPOCH: 187, total reward: 2.820706367492676, timestep: 25, epsilon: 0.6259999999999997\n",
      "EPOCH: 188, total reward: 94.4786376953125, timestep: 154, epsilon: 0.6239999999999997\n",
      "EPOCH: 189, total reward: 3.577709197998047, timestep: 24, epsilon: 0.6219999999999997\n",
      "EPOCH: 190, total reward: 19.621749877929688, timestep: 39, epsilon: 0.6199999999999997\n",
      "EPOCH: 191, total reward: 2.93416690826416, timestep: 27, epsilon: 0.6179999999999997\n",
      "EPOCH: 192, total reward: 52.919189453125, timestep: 92, epsilon: 0.6159999999999997\n",
      "EPOCH: 193, total reward: 2.9064722061157227, timestep: 23, epsilon: 0.6139999999999997\n",
      "EPOCH: 194, total reward: 52.867027282714844, timestep: 92, epsilon: 0.6119999999999997\n",
      "EPOCH: 195, total reward: 61.29145812988281, timestep: 102, epsilon: 0.6099999999999997\n",
      "EPOCH: 196, total reward: 61.222084045410156, timestep: 100, epsilon: 0.6079999999999997\n",
      "EPOCH: 197, total reward: 27.95090103149414, timestep: 52, epsilon: 0.6059999999999997\n",
      "EPOCH: 198, total reward: 3.748748779296875, timestep: 24, epsilon: 0.6039999999999996\n",
      "EPOCH: 199, total reward: 44.60216522216797, timestep: 89, epsilon: 0.6019999999999996\n",
      "EPOCH: 200, total reward: 11.191038131713867, timestep: 28, epsilon: 0.5999999999999996\n",
      "EPOCH: 201, total reward: 11.31144905090332, timestep: 41, epsilon: 0.5979999999999996\n",
      "EPOCH: 202, total reward: 3.557842254638672, timestep: 22, epsilon: 0.5959999999999996\n",
      "EPOCH: 203, total reward: 11.322664260864258, timestep: 30, epsilon: 0.5939999999999996\n",
      "EPOCH: 204, total reward: 2.9650182723999023, timestep: 23, epsilon: 0.5919999999999996\n",
      "EPOCH: 205, total reward: 2.85162353515625, timestep: 27, epsilon: 0.5899999999999996\n",
      "EPOCH: 206, total reward: 2.9380903244018555, timestep: 26, epsilon: 0.5879999999999996\n",
      "EPOCH: 207, total reward: 12.024550437927246, timestep: 39, epsilon: 0.5859999999999996\n",
      "EPOCH: 208, total reward: 2.838754653930664, timestep: 29, epsilon: 0.5839999999999996\n",
      "EPOCH: 209, total reward: 3.442070960998535, timestep: 23, epsilon: 0.5819999999999996\n",
      "EPOCH: 210, total reward: 2.8983306884765625, timestep: 29, epsilon: 0.5799999999999996\n",
      "EPOCH: 211, total reward: 52.875572204589844, timestep: 90, epsilon: 0.5779999999999996\n",
      "EPOCH: 212, total reward: 3.679051399230957, timestep: 23, epsilon: 0.5759999999999996\n",
      "EPOCH: 213, total reward: 0.0, timestep: 2, epsilon: 0.5739999999999996\n",
      "EPOCH: 214, total reward: 61.149017333984375, timestep: 103, epsilon: 0.5719999999999996\n",
      "EPOCH: 215, total reward: 36.17649841308594, timestep: 63, epsilon: 0.5699999999999996\n",
      "EPOCH: 216, total reward: 3.0501937866210938, timestep: 26, epsilon: 0.5679999999999996\n",
      "EPOCH: 217, total reward: 27.919387817382812, timestep: 54, epsilon: 0.5659999999999996\n",
      "EPOCH: 218, total reward: 19.568449020385742, timestep: 38, epsilon: 0.5639999999999996\n",
      "EPOCH: 219, total reward: 3.6087303161621094, timestep: 24, epsilon: 0.5619999999999996\n",
      "EPOCH: 220, total reward: 19.563262939453125, timestep: 43, epsilon: 0.5599999999999996\n",
      "EPOCH: 221, total reward: 2.887449264526367, timestep: 25, epsilon: 0.5579999999999996\n",
      "EPOCH: 222, total reward: 19.513813018798828, timestep: 51, epsilon: 0.5559999999999996\n",
      "EPOCH: 223, total reward: 3.6936159133911133, timestep: 25, epsilon: 0.5539999999999996\n",
      "EPOCH: 224, total reward: 27.901073455810547, timestep: 55, epsilon: 0.5519999999999996\n",
      "EPOCH: 225, total reward: 11.151379585266113, timestep: 38, epsilon: 0.5499999999999996\n",
      "EPOCH: 226, total reward: 86.2428207397461, timestep: 139, epsilon: 0.5479999999999996\n",
      "EPOCH: 227, total reward: 2.8713455200195312, timestep: 26, epsilon: 0.5459999999999996\n",
      "EPOCH: 228, total reward: 11.967610359191895, timestep: 37, epsilon: 0.5439999999999996\n",
      "EPOCH: 229, total reward: 36.23362731933594, timestep: 67, epsilon: 0.5419999999999996\n",
      "EPOCH: 230, total reward: 3.674565315246582, timestep: 24, epsilon: 0.5399999999999996\n",
      "EPOCH: 231, total reward: 19.634098052978516, timestep: 42, epsilon: 0.5379999999999996\n",
      "EPOCH: 232, total reward: 3.2159366607666016, timestep: 23, epsilon: 0.5359999999999996\n",
      "EPOCH: 233, total reward: 19.51103973388672, timestep: 42, epsilon: 0.5339999999999996\n",
      "EPOCH: 234, total reward: 69.5089111328125, timestep: 127, epsilon: 0.5319999999999996\n",
      "EPOCH: 235, total reward: 44.500511169433594, timestep: 89, epsilon: 0.5299999999999996\n",
      "EPOCH: 236, total reward: 3.6515684127807617, timestep: 23, epsilon: 0.5279999999999996\n",
      "EPOCH: 237, total reward: 27.839303970336914, timestep: 55, epsilon: 0.5259999999999996\n",
      "EPOCH: 238, total reward: 2.9951181411743164, timestep: 23, epsilon: 0.5239999999999996\n",
      "EPOCH: 239, total reward: 19.61261749267578, timestep: 40, epsilon: 0.5219999999999996\n",
      "EPOCH: 240, total reward: 0.0, timestep: 1, epsilon: 0.5199999999999996\n",
      "EPOCH: 241, total reward: 77.90744018554688, timestep: 126, epsilon: 0.5179999999999996\n",
      "EPOCH: 242, total reward: 0.0, timestep: 3, epsilon: 0.5159999999999996\n",
      "EPOCH: 243, total reward: 44.519142150878906, timestep: 75, epsilon: 0.5139999999999996\n",
      "EPOCH: 244, total reward: 0.0, timestep: 3, epsilon: 0.5119999999999996\n",
      "EPOCH: 245, total reward: 2.8933444023132324, timestep: 26, epsilon: 0.5099999999999996\n",
      "EPOCH: 246, total reward: 0.0, timestep: 4, epsilon: 0.5079999999999996\n",
      "EPOCH: 247, total reward: 19.52140235900879, timestep: 50, epsilon: 0.5059999999999996\n",
      "EPOCH: 248, total reward: 3.723738670349121, timestep: 23, epsilon: 0.5039999999999996\n",
      "EPOCH: 249, total reward: 27.86394500732422, timestep: 54, epsilon: 0.5019999999999996\n",
      "EPOCH: 250, total reward: 19.541980743408203, timestep: 50, epsilon: 0.49999999999999956\n",
      "EPOCH: 251, total reward: 2.8387327194213867, timestep: 25, epsilon: 0.49799999999999955\n",
      "EPOCH: 252, total reward: 19.54656982421875, timestep: 42, epsilon: 0.49599999999999955\n",
      "EPOCH: 253, total reward: 3.0753889083862305, timestep: 23, epsilon: 0.49399999999999955\n",
      "EPOCH: 254, total reward: 69.5401611328125, timestep: 115, epsilon: 0.49199999999999955\n",
      "EPOCH: 255, total reward: 2.9422447681427, timestep: 26, epsilon: 0.48999999999999955\n",
      "EPOCH: 256, total reward: 27.875774383544922, timestep: 55, epsilon: 0.48799999999999955\n",
      "EPOCH: 257, total reward: 0.0, timestep: 6, epsilon: 0.48599999999999954\n",
      "EPOCH: 258, total reward: 19.505760192871094, timestep: 46, epsilon: 0.48399999999999954\n",
      "EPOCH: 259, total reward: 0.0, timestep: 3, epsilon: 0.48199999999999954\n",
      "EPOCH: 260, total reward: 2.9466333389282227, timestep: 22, epsilon: 0.47999999999999954\n",
      "EPOCH: 261, total reward: 61.176673889160156, timestep: 102, epsilon: 0.47799999999999954\n",
      "EPOCH: 262, total reward: 27.840198516845703, timestep: 52, epsilon: 0.47599999999999953\n",
      "EPOCH: 263, total reward: 2.9134035110473633, timestep: 26, epsilon: 0.47399999999999953\n",
      "EPOCH: 264, total reward: 36.16204833984375, timestep: 65, epsilon: 0.47199999999999953\n",
      "EPOCH: 265, total reward: 19.635995864868164, timestep: 38, epsilon: 0.46999999999999953\n",
      "EPOCH: 266, total reward: 19.486312866210938, timestep: 40, epsilon: 0.4679999999999995\n",
      "EPOCH: 267, total reward: 0.0, timestep: 1, epsilon: 0.4659999999999995\n",
      "EPOCH: 268, total reward: 69.55130767822266, timestep: 112, epsilon: 0.4639999999999995\n",
      "EPOCH: 269, total reward: 11.247401237487793, timestep: 29, epsilon: 0.4619999999999995\n",
      "EPOCH: 270, total reward: 36.16050720214844, timestep: 64, epsilon: 0.4599999999999995\n",
      "EPOCH: 271, total reward: 27.89815902709961, timestep: 51, epsilon: 0.4579999999999995\n",
      "EPOCH: 272, total reward: 3.490550994873047, timestep: 23, epsilon: 0.4559999999999995\n",
      "EPOCH: 273, total reward: 19.478914260864258, timestep: 42, epsilon: 0.4539999999999995\n",
      "EPOCH: 274, total reward: 44.53778839111328, timestep: 75, epsilon: 0.4519999999999995\n",
      "EPOCH: 275, total reward: 2.8265790939331055, timestep: 29, epsilon: 0.4499999999999995\n",
      "EPOCH: 276, total reward: 3.3564367294311523, timestep: 23, epsilon: 0.4479999999999995\n",
      "EPOCH: 277, total reward: 36.27513885498047, timestep: 66, epsilon: 0.4459999999999995\n",
      "EPOCH: 278, total reward: 3.1134538650512695, timestep: 25, epsilon: 0.4439999999999995\n",
      "EPOCH: 279, total reward: 2.8824501037597656, timestep: 29, epsilon: 0.4419999999999995\n",
      "EPOCH: 280, total reward: 27.865251541137695, timestep: 51, epsilon: 0.4399999999999995\n",
      "EPOCH: 281, total reward: 3.692702293395996, timestep: 24, epsilon: 0.4379999999999995\n",
      "EPOCH: 282, total reward: 0.0, timestep: 2, epsilon: 0.4359999999999995\n",
      "EPOCH: 283, total reward: 36.15174102783203, timestep: 65, epsilon: 0.4339999999999995\n",
      "EPOCH: 284, total reward: 44.57110595703125, timestep: 78, epsilon: 0.4319999999999995\n",
      "EPOCH: 285, total reward: 27.81252098083496, timestep: 50, epsilon: 0.4299999999999995\n",
      "EPOCH: 286, total reward: 19.525794982910156, timestep: 40, epsilon: 0.4279999999999995\n",
      "EPOCH: 287, total reward: 19.483827590942383, timestep: 39, epsilon: 0.4259999999999995\n",
      "EPOCH: 288, total reward: 19.48835563659668, timestep: 38, epsilon: 0.4239999999999995\n",
      "EPOCH: 289, total reward: 44.523807525634766, timestep: 74, epsilon: 0.4219999999999995\n",
      "EPOCH: 290, total reward: 19.5279483795166, timestep: 41, epsilon: 0.4199999999999995\n",
      "EPOCH: 291, total reward: 19.545019149780273, timestep: 38, epsilon: 0.4179999999999995\n",
      "EPOCH: 292, total reward: 0.0, timestep: 2, epsilon: 0.4159999999999995\n",
      "EPOCH: 293, total reward: 36.253929138183594, timestep: 66, epsilon: 0.4139999999999995\n",
      "EPOCH: 294, total reward: 3.0474395751953125, timestep: 23, epsilon: 0.4119999999999995\n",
      "EPOCH: 295, total reward: 11.14328384399414, timestep: 29, epsilon: 0.4099999999999995\n",
      "EPOCH: 296, total reward: 3.3126134872436523, timestep: 24, epsilon: 0.4079999999999995\n",
      "EPOCH: 297, total reward: 11.347832679748535, timestep: 29, epsilon: 0.4059999999999995\n",
      "EPOCH: 298, total reward: 2.9625844955444336, timestep: 25, epsilon: 0.40399999999999947\n",
      "EPOCH: 299, total reward: 36.17967987060547, timestep: 65, epsilon: 0.40199999999999947\n",
      "EPOCH: 300, total reward: 2.887892723083496, timestep: 24, epsilon: 0.39999999999999947\n",
      "EPOCH: 301, total reward: 19.50074005126953, timestep: 41, epsilon: 0.39799999999999947\n",
      "EPOCH: 302, total reward: 11.941819190979004, timestep: 36, epsilon: 0.39599999999999946\n",
      "EPOCH: 303, total reward: 11.201139450073242, timestep: 30, epsilon: 0.39399999999999946\n",
      "EPOCH: 304, total reward: 2.866283893585205, timestep: 25, epsilon: 0.39199999999999946\n",
      "EPOCH: 305, total reward: 0.0, timestep: 2, epsilon: 0.38999999999999946\n",
      "EPOCH: 306, total reward: 19.51378059387207, timestep: 40, epsilon: 0.38799999999999946\n",
      "EPOCH: 307, total reward: 2.9077634811401367, timestep: 24, epsilon: 0.38599999999999945\n",
      "EPOCH: 308, total reward: 11.28212833404541, timestep: 28, epsilon: 0.38399999999999945\n",
      "EPOCH: 309, total reward: 3.1950511932373047, timestep: 24, epsilon: 0.38199999999999945\n",
      "EPOCH: 310, total reward: 11.217562675476074, timestep: 27, epsilon: 0.37999999999999945\n",
      "EPOCH: 311, total reward: 2.9945449829101562, timestep: 26, epsilon: 0.37799999999999945\n",
      "EPOCH: 312, total reward: 28.015594482421875, timestep: 52, epsilon: 0.37599999999999945\n",
      "EPOCH: 313, total reward: 2.855738639831543, timestep: 26, epsilon: 0.37399999999999944\n",
      "EPOCH: 314, total reward: 11.278066635131836, timestep: 29, epsilon: 0.37199999999999944\n",
      "EPOCH: 315, total reward: 27.961181640625, timestep: 52, epsilon: 0.36999999999999944\n",
      "EPOCH: 316, total reward: 28.363216400146484, timestep: 48, epsilon: 0.36799999999999944\n",
      "EPOCH: 317, total reward: 27.852346420288086, timestep: 54, epsilon: 0.36599999999999944\n",
      "EPOCH: 318, total reward: 3.142184257507324, timestep: 24, epsilon: 0.36399999999999944\n",
      "EPOCH: 319, total reward: 44.573177337646484, timestep: 93, epsilon: 0.36199999999999943\n",
      "EPOCH: 320, total reward: 11.806449890136719, timestep: 48, epsilon: 0.35999999999999943\n",
      "EPOCH: 321, total reward: 19.480247497558594, timestep: 53, epsilon: 0.35799999999999943\n",
      "EPOCH: 322, total reward: 36.195640563964844, timestep: 104, epsilon: 0.35599999999999943\n",
      "EPOCH: 323, total reward: 2.8359718322753906, timestep: 36, epsilon: 0.3539999999999994\n",
      "EPOCH: 324, total reward: 27.815753936767578, timestep: 91, epsilon: 0.3519999999999994\n",
      "EPOCH: 325, total reward: 3.40706729888916, timestep: 36, epsilon: 0.3499999999999994\n",
      "EPOCH: 326, total reward: 27.967571258544922, timestep: 89, epsilon: 0.3479999999999994\n",
      "EPOCH: 327, total reward: 2.820206642150879, timestep: 28, epsilon: 0.3459999999999994\n",
      "EPOCH: 328, total reward: 52.918968200683594, timestep: 92, epsilon: 0.3439999999999994\n",
      "EPOCH: 329, total reward: 44.61846923828125, timestep: 86, epsilon: 0.3419999999999994\n",
      "EPOCH: 330, total reward: 3.162982940673828, timestep: 26, epsilon: 0.3399999999999994\n",
      "EPOCH: 331, total reward: 27.878555297851562, timestep: 53, epsilon: 0.3379999999999994\n",
      "EPOCH: 332, total reward: 11.3427095413208, timestep: 27, epsilon: 0.3359999999999994\n",
      "EPOCH: 333, total reward: 20.157203674316406, timestep: 36, epsilon: 0.3339999999999994\n",
      "EPOCH: 334, total reward: 11.225143432617188, timestep: 29, epsilon: 0.3319999999999994\n",
      "EPOCH: 335, total reward: 3.6561851501464844, timestep: 24, epsilon: 0.3299999999999994\n",
      "EPOCH: 336, total reward: 11.282960891723633, timestep: 30, epsilon: 0.3279999999999994\n",
      "EPOCH: 337, total reward: 2.8297343254089355, timestep: 25, epsilon: 0.3259999999999994\n",
      "EPOCH: 338, total reward: 2.8488869667053223, timestep: 25, epsilon: 0.3239999999999994\n",
      "EPOCH: 339, total reward: 0.0, timestep: 2, epsilon: 0.3219999999999994\n",
      "EPOCH: 340, total reward: 19.516952514648438, timestep: 39, epsilon: 0.3199999999999994\n",
      "EPOCH: 341, total reward: 19.537559509277344, timestep: 38, epsilon: 0.3179999999999994\n",
      "EPOCH: 342, total reward: 19.64636993408203, timestep: 42, epsilon: 0.3159999999999994\n",
      "EPOCH: 343, total reward: 19.526657104492188, timestep: 38, epsilon: 0.3139999999999994\n",
      "EPOCH: 344, total reward: 3.3230791091918945, timestep: 24, epsilon: 0.3119999999999994\n",
      "EPOCH: 345, total reward: 19.54192352294922, timestep: 42, epsilon: 0.3099999999999994\n",
      "EPOCH: 346, total reward: 3.68735408782959, timestep: 24, epsilon: 0.3079999999999994\n",
      "EPOCH: 347, total reward: 11.152761459350586, timestep: 27, epsilon: 0.3059999999999994\n",
      "EPOCH: 348, total reward: 19.634387969970703, timestep: 42, epsilon: 0.3039999999999994\n",
      "EPOCH: 349, total reward: 0.0, timestep: 2, epsilon: 0.3019999999999994\n",
      "EPOCH: 350, total reward: 11.3427095413208, timestep: 26, epsilon: 0.2999999999999994\n",
      "EPOCH: 351, total reward: 19.620952606201172, timestep: 38, epsilon: 0.2979999999999994\n",
      "EPOCH: 352, total reward: 27.94091796875, timestep: 49, epsilon: 0.2959999999999994\n",
      "EPOCH: 353, total reward: 11.3427095413208, timestep: 26, epsilon: 0.2939999999999994\n",
      "EPOCH: 354, total reward: 11.251328468322754, timestep: 29, epsilon: 0.29199999999999937\n",
      "EPOCH: 355, total reward: 19.621131896972656, timestep: 37, epsilon: 0.28999999999999937\n",
      "EPOCH: 356, total reward: 19.593082427978516, timestep: 38, epsilon: 0.28799999999999937\n",
      "EPOCH: 357, total reward: 19.62322235107422, timestep: 39, epsilon: 0.28599999999999937\n",
      "EPOCH: 358, total reward: 19.569257736206055, timestep: 39, epsilon: 0.28399999999999936\n",
      "EPOCH: 359, total reward: 11.25416088104248, timestep: 28, epsilon: 0.28199999999999936\n",
      "EPOCH: 360, total reward: 27.868820190429688, timestep: 51, epsilon: 0.27999999999999936\n",
      "EPOCH: 361, total reward: 19.53775405883789, timestep: 39, epsilon: 0.27799999999999936\n",
      "EPOCH: 362, total reward: 11.245399475097656, timestep: 28, epsilon: 0.27599999999999936\n",
      "EPOCH: 363, total reward: 27.85722541809082, timestep: 49, epsilon: 0.27399999999999936\n",
      "EPOCH: 364, total reward: 11.3427095413208, timestep: 26, epsilon: 0.27199999999999935\n",
      "EPOCH: 365, total reward: 11.28931713104248, timestep: 28, epsilon: 0.26999999999999935\n",
      "EPOCH: 366, total reward: 19.549779891967773, timestep: 37, epsilon: 0.26799999999999935\n",
      "EPOCH: 367, total reward: 20.398326873779297, timestep: 38, epsilon: 0.26599999999999935\n",
      "EPOCH: 368, total reward: 11.311697006225586, timestep: 28, epsilon: 0.26399999999999935\n",
      "EPOCH: 369, total reward: 19.514772415161133, timestep: 41, epsilon: 0.26199999999999934\n",
      "EPOCH: 370, total reward: 11.3427095413208, timestep: 26, epsilon: 0.25999999999999934\n",
      "EPOCH: 371, total reward: 2.916400909423828, timestep: 24, epsilon: 0.25799999999999934\n",
      "EPOCH: 372, total reward: 19.531431198120117, timestep: 38, epsilon: 0.25599999999999934\n",
      "EPOCH: 373, total reward: 11.214584350585938, timestep: 28, epsilon: 0.25399999999999934\n",
      "EPOCH: 374, total reward: 19.532852172851562, timestep: 39, epsilon: 0.25199999999999934\n",
      "EPOCH: 375, total reward: 19.538257598876953, timestep: 38, epsilon: 0.24999999999999933\n",
      "EPOCH: 376, total reward: 19.49869728088379, timestep: 40, epsilon: 0.24799999999999933\n",
      "EPOCH: 377, total reward: 19.499563217163086, timestep: 38, epsilon: 0.24599999999999933\n",
      "EPOCH: 378, total reward: 0.0, timestep: 2, epsilon: 0.24399999999999933\n",
      "EPOCH: 379, total reward: 20.135604858398438, timestep: 36, epsilon: 0.24199999999999933\n",
      "EPOCH: 380, total reward: 11.238066673278809, timestep: 27, epsilon: 0.23999999999999932\n",
      "EPOCH: 381, total reward: 27.96410369873047, timestep: 55, epsilon: 0.23799999999999932\n",
      "EPOCH: 382, total reward: 2.815044403076172, timestep: 23, epsilon: 0.23599999999999932\n",
      "EPOCH: 383, total reward: 19.548965454101562, timestep: 39, epsilon: 0.23399999999999932\n",
      "EPOCH: 384, total reward: 19.536327362060547, timestep: 41, epsilon: 0.23199999999999932\n",
      "EPOCH: 385, total reward: 2.887775421142578, timestep: 28, epsilon: 0.22999999999999932\n",
      "EPOCH: 386, total reward: 2.816025733947754, timestep: 23, epsilon: 0.22799999999999931\n",
      "EPOCH: 387, total reward: 27.973403930664062, timestep: 51, epsilon: 0.2259999999999993\n",
      "EPOCH: 388, total reward: 61.29274368286133, timestep: 104, epsilon: 0.2239999999999993\n",
      "EPOCH: 389, total reward: 36.179534912109375, timestep: 75, epsilon: 0.2219999999999993\n",
      "EPOCH: 390, total reward: 2.914483070373535, timestep: 26, epsilon: 0.2199999999999993\n",
      "EPOCH: 391, total reward: 19.585010528564453, timestep: 54, epsilon: 0.2179999999999993\n",
      "EPOCH: 392, total reward: 11.149993896484375, timestep: 39, epsilon: 0.2159999999999993\n",
      "EPOCH: 393, total reward: 2.831294059753418, timestep: 23, epsilon: 0.2139999999999993\n",
      "EPOCH: 394, total reward: 19.478981018066406, timestep: 42, epsilon: 0.2119999999999993\n",
      "EPOCH: 395, total reward: 44.5513916015625, timestep: 87, epsilon: 0.2099999999999993\n",
      "EPOCH: 396, total reward: 19.47887420654297, timestep: 42, epsilon: 0.2079999999999993\n",
      "EPOCH: 397, total reward: 102.85667419433594, timestep: 177, epsilon: 0.2059999999999993\n",
      "EPOCH: 398, total reward: 2.813753128051758, timestep: 26, epsilon: 0.2039999999999993\n",
      "EPOCH: 399, total reward: 2.8403940200805664, timestep: 28, epsilon: 0.2019999999999993\n",
      "EPOCH: 400, total reward: 2.915297508239746, timestep: 26, epsilon: 0.1999999999999993\n",
      "EPOCH: 401, total reward: 19.56426239013672, timestep: 51, epsilon: 0.1979999999999993\n",
      "EPOCH: 402, total reward: 2.9445247650146484, timestep: 26, epsilon: 0.19599999999999929\n",
      "EPOCH: 403, total reward: 36.344642639160156, timestep: 62, epsilon: 0.19399999999999928\n",
      "EPOCH: 404, total reward: 27.82525634765625, timestep: 52, epsilon: 0.19199999999999928\n",
      "EPOCH: 405, total reward: 19.573017120361328, timestep: 40, epsilon: 0.18999999999999928\n",
      "EPOCH: 406, total reward: 36.20048522949219, timestep: 63, epsilon: 0.18799999999999928\n",
      "EPOCH: 407, total reward: 27.92780876159668, timestep: 52, epsilon: 0.18599999999999928\n",
      "EPOCH: 408, total reward: 19.4871768951416, timestep: 39, epsilon: 0.18399999999999928\n",
      "EPOCH: 409, total reward: 11.155153274536133, timestep: 27, epsilon: 0.18199999999999927\n",
      "EPOCH: 410, total reward: 11.3427095413208, timestep: 26, epsilon: 0.17999999999999927\n",
      "EPOCH: 411, total reward: 19.519075393676758, timestep: 37, epsilon: 0.17799999999999927\n",
      "EPOCH: 412, total reward: 19.56016731262207, timestep: 40, epsilon: 0.17599999999999927\n",
      "EPOCH: 413, total reward: 19.954198837280273, timestep: 37, epsilon: 0.17399999999999927\n",
      "EPOCH: 414, total reward: 11.26816463470459, timestep: 26, epsilon: 0.17199999999999926\n",
      "EPOCH: 415, total reward: 19.637798309326172, timestep: 41, epsilon: 0.16999999999999926\n",
      "EPOCH: 416, total reward: 11.264751434326172, timestep: 28, epsilon: 0.16799999999999926\n",
      "EPOCH: 417, total reward: 0.0, timestep: 2, epsilon: 0.16599999999999926\n",
      "EPOCH: 418, total reward: 2.9407758712768555, timestep: 23, epsilon: 0.16399999999999926\n",
      "EPOCH: 419, total reward: 11.188189506530762, timestep: 29, epsilon: 0.16199999999999926\n",
      "EPOCH: 420, total reward: 0.0, timestep: 2, epsilon: 0.15999999999999925\n",
      "EPOCH: 421, total reward: 19.919296264648438, timestep: 36, epsilon: 0.15799999999999925\n",
      "EPOCH: 422, total reward: 11.3427095413208, timestep: 26, epsilon: 0.15599999999999925\n",
      "EPOCH: 423, total reward: 0.0, timestep: 1, epsilon: 0.15399999999999925\n",
      "EPOCH: 424, total reward: 11.2824125289917, timestep: 28, epsilon: 0.15199999999999925\n",
      "EPOCH: 425, total reward: 27.843948364257812, timestep: 52, epsilon: 0.14999999999999925\n",
      "EPOCH: 426, total reward: 27.898868560791016, timestep: 50, epsilon: 0.14799999999999924\n",
      "EPOCH: 427, total reward: 27.900760650634766, timestep: 53, epsilon: 0.14599999999999924\n",
      "EPOCH: 428, total reward: 27.954315185546875, timestep: 52, epsilon: 0.14399999999999924\n",
      "EPOCH: 429, total reward: 2.9086618423461914, timestep: 26, epsilon: 0.14199999999999924\n",
      "EPOCH: 430, total reward: 2.919342517852783, timestep: 26, epsilon: 0.13999999999999924\n",
      "EPOCH: 431, total reward: 2.945035457611084, timestep: 26, epsilon: 0.13799999999999923\n",
      "EPOCH: 432, total reward: 2.843996047973633, timestep: 25, epsilon: 0.13599999999999923\n",
      "EPOCH: 433, total reward: 2.8900041580200195, timestep: 26, epsilon: 0.13399999999999923\n",
      "EPOCH: 434, total reward: 2.815201759338379, timestep: 27, epsilon: 0.13199999999999923\n",
      "EPOCH: 435, total reward: 2.931748390197754, timestep: 26, epsilon: 0.12999999999999923\n",
      "EPOCH: 436, total reward: 53.516082763671875, timestep: 113, epsilon: 0.12799999999999923\n",
      "EPOCH: 437, total reward: 27.855609893798828, timestep: 77, epsilon: 0.12599999999999922\n",
      "EPOCH: 438, total reward: 11.362621307373047, timestep: 51, epsilon: 0.12399999999999922\n",
      "EPOCH: 439, total reward: 11.186066627502441, timestep: 53, epsilon: 0.12199999999999922\n",
      "EPOCH: 440, total reward: 3.749030113220215, timestep: 37, epsilon: 0.11999999999999922\n",
      "EPOCH: 441, total reward: 27.855791091918945, timestep: 91, epsilon: 0.11799999999999922\n",
      "EPOCH: 442, total reward: 11.285008430480957, timestep: 53, epsilon: 0.11599999999999921\n",
      "EPOCH: 443, total reward: 2.8189620971679688, timestep: 37, epsilon: 0.11399999999999921\n",
      "EPOCH: 444, total reward: 2.885274887084961, timestep: 40, epsilon: 0.11199999999999921\n",
      "EPOCH: 445, total reward: 2.902475357055664, timestep: 39, epsilon: 0.10999999999999921\n",
      "EPOCH: 446, total reward: 27.961833953857422, timestep: 64, epsilon: 0.10799999999999921\n",
      "EPOCH: 447, total reward: 11.356382369995117, timestep: 49, epsilon: 0.1059999999999992\n",
      "EPOCH: 448, total reward: 2.9365062713623047, timestep: 29, epsilon: 0.1039999999999992\n",
      "EPOCH: 449, total reward: 44.570350646972656, timestep: 90, epsilon: 0.1019999999999992\n",
      "EPOCH: 450, total reward: 252.8861083984375, timestep: 403, epsilon: 0.0999999999999992\n",
      "EPOCH: 451, total reward: 69.62146759033203, timestep: 127, epsilon: 0.0999999999999992\n",
      "EPOCH: 452, total reward: 19.476722717285156, timestep: 51, epsilon: 0.0999999999999992\n",
      "EPOCH: 453, total reward: 2.887342929840088, timestep: 25, epsilon: 0.0999999999999992\n",
      "EPOCH: 454, total reward: 27.90726089477539, timestep: 53, epsilon: 0.0999999999999992\n",
      "EPOCH: 455, total reward: 2.913555145263672, timestep: 24, epsilon: 0.0999999999999992\n",
      "EPOCH: 456, total reward: 27.836837768554688, timestep: 51, epsilon: 0.0999999999999992\n",
      "EPOCH: 457, total reward: 27.96774673461914, timestep: 52, epsilon: 0.0999999999999992\n",
      "EPOCH: 458, total reward: 27.899919509887695, timestep: 52, epsilon: 0.0999999999999992\n",
      "EPOCH: 459, total reward: 19.545150756835938, timestep: 39, epsilon: 0.0999999999999992\n",
      "EPOCH: 460, total reward: 11.227349281311035, timestep: 28, epsilon: 0.0999999999999992\n",
      "EPOCH: 461, total reward: 2.858107089996338, timestep: 25, epsilon: 0.0999999999999992\n",
      "EPOCH: 462, total reward: 20.128660202026367, timestep: 36, epsilon: 0.0999999999999992\n",
      "EPOCH: 463, total reward: 19.57778549194336, timestep: 40, epsilon: 0.0999999999999992\n",
      "EPOCH: 464, total reward: 11.261128425598145, timestep: 28, epsilon: 0.0999999999999992\n",
      "EPOCH: 465, total reward: 19.641002655029297, timestep: 36, epsilon: 0.0999999999999992\n",
      "EPOCH: 466, total reward: 11.3427095413208, timestep: 26, epsilon: 0.0999999999999992\n",
      "EPOCH: 467, total reward: 0.0, timestep: 1, epsilon: 0.0999999999999992\n",
      "EPOCH: 468, total reward: 11.3427095413208, timestep: 26, epsilon: 0.0999999999999992\n",
      "EPOCH: 469, total reward: 0.0, timestep: 1, epsilon: 0.0999999999999992\n",
      "EPOCH: 470, total reward: 11.237113952636719, timestep: 27, epsilon: 0.0999999999999992\n",
      "EPOCH: 471, total reward: 11.144174575805664, timestep: 28, epsilon: 0.0999999999999992\n",
      "EPOCH: 472, total reward: 19.6612606048584, timestep: 37, epsilon: 0.0999999999999992\n",
      "EPOCH: 473, total reward: 11.316899299621582, timestep: 28, epsilon: 0.0999999999999992\n",
      "EPOCH: 474, total reward: 11.3427095413208, timestep: 26, epsilon: 0.0999999999999992\n",
      "EPOCH: 475, total reward: 19.595108032226562, timestep: 40, epsilon: 0.0999999999999992\n",
      "EPOCH: 476, total reward: 19.48439598083496, timestep: 37, epsilon: 0.0999999999999992\n",
      "EPOCH: 477, total reward: 11.232036590576172, timestep: 28, epsilon: 0.0999999999999992\n",
      "EPOCH: 478, total reward: 2.913630485534668, timestep: 23, epsilon: 0.0999999999999992\n",
      "EPOCH: 479, total reward: 0.0, timestep: 2, epsilon: 0.0999999999999992\n",
      "EPOCH: 480, total reward: 3.2714738845825195, timestep: 25, epsilon: 0.0999999999999992\n",
      "EPOCH: 481, total reward: 0.0, timestep: 1, epsilon: 0.0999999999999992\n",
      "EPOCH: 482, total reward: 36.18888854980469, timestep: 66, epsilon: 0.0999999999999992\n",
      "EPOCH: 483, total reward: 2.8509812355041504, timestep: 25, epsilon: 0.0999999999999992\n",
      "EPOCH: 484, total reward: 44.564697265625, timestep: 77, epsilon: 0.0999999999999992\n",
      "EPOCH: 485, total reward: 36.149810791015625, timestep: 65, epsilon: 0.0999999999999992\n",
      "EPOCH: 486, total reward: 11.15592098236084, timestep: 39, epsilon: 0.0999999999999992\n",
      "EPOCH: 487, total reward: 2.9223203659057617, timestep: 24, epsilon: 0.0999999999999992\n",
      "EPOCH: 488, total reward: 27.970428466796875, timestep: 55, epsilon: 0.0999999999999992\n",
      "EPOCH: 489, total reward: 2.923208236694336, timestep: 24, epsilon: 0.0999999999999992\n",
      "EPOCH: 490, total reward: 28.005447387695312, timestep: 50, epsilon: 0.0999999999999992\n",
      "EPOCH: 491, total reward: 19.63929557800293, timestep: 42, epsilon: 0.0999999999999992\n",
      "EPOCH: 492, total reward: 2.815746307373047, timestep: 24, epsilon: 0.0999999999999992\n",
      "EPOCH: 493, total reward: 11.181291580200195, timestep: 28, epsilon: 0.0999999999999992\n",
      "EPOCH: 494, total reward: 19.635189056396484, timestep: 39, epsilon: 0.0999999999999992\n",
      "EPOCH: 495, total reward: 19.49669647216797, timestep: 39, epsilon: 0.0999999999999992\n",
      "EPOCH: 496, total reward: 19.587932586669922, timestep: 37, epsilon: 0.0999999999999992\n",
      "EPOCH: 497, total reward: 11.3427095413208, timestep: 26, epsilon: 0.0999999999999992\n",
      "EPOCH: 498, total reward: 11.186041831970215, timestep: 27, epsilon: 0.0999999999999992\n",
      "EPOCH: 499, total reward: 11.161561012268066, timestep: 28, epsilon: 0.0999999999999992\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 500\n",
    "mem_size = 10000\n",
    "EPSILON = 1\n",
    "MIN_EPSILON = .1\n",
    "MAX_STEP = 5000\n",
    "\n",
    "MIN_BATCH = 256\n",
    "GAMMA = 0.9\n",
    "\n",
    "losses = []\n",
    "important_experiences = []\n",
    "experiences = deque(maxlen=mem_size)\n",
    "\n",
    "for i in range(EPOCH):\n",
    "    env.reset()\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "\n",
    "    j = 0\n",
    "    epoch_rewards = []\n",
    "    while j < MAX_STEP:\n",
    "        j += 1\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "        if (len(terminal_steps) > 0):\n",
    "            break\n",
    "\n",
    "        state = torch.Tensor(preprocess_input(decision_steps))\n",
    "\n",
    "        qval = model(state)\n",
    "        if np.random.rand() > EPSILON:\n",
    "            # exploit\n",
    "            action = np.argmax(qval.detach().numpy())\n",
    "        else:\n",
    "            # explore\n",
    "            action = np.random.randint(0, 2)\n",
    "\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(np.array([[action]]))\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "\n",
    "        new_decision_steps, new_terminal_steps = env.get_steps(behavior_name)\n",
    "        current_step = new_decision_steps if len(new_terminal_steps) == 0 else new_terminal_steps\n",
    "        reward = current_step.reward[0]\n",
    "\n",
    "        if reward == 0:\n",
    "            continue\n",
    "\n",
    "        epoch_rewards.append(reward)\n",
    "    \n",
    "        state2 = torch.Tensor(preprocess_input(current_step))\n",
    "        done = len(new_terminal_steps) > 0\n",
    "\n",
    "        current_exp = (state, action, reward, state2, done)\n",
    "\n",
    "        if done:\n",
    "            important_experiences.append(current_exp)\n",
    "        else:\n",
    "            experiences.append(current_exp)\n",
    "\n",
    "        all_exp = experiences.copy()\n",
    "        all_exp.extend(important_experiences)\n",
    "\n",
    "        if len(all_exp) >= MIN_BATCH:\n",
    "            batch = random.sample(all_exp, MIN_BATCH)\n",
    "\n",
    "            states = torch.cat([s for (s, a, r, s2, done) in batch])\n",
    "            actions = torch.Tensor([a for (s, a, r, s2, done) in batch])\n",
    "            states2 = torch.cat([s2 for (s, a, r, s2, done) in batch])\n",
    "            done_data = torch.Tensor([done for (s, a, r, s2, done) in batch])\n",
    "            rewards = torch.Tensor([r for (s, a, r, s2, done) in batch])\n",
    "\n",
    "            qvals = model(states)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                qvals_2 = model(states2)\n",
    "\n",
    "            target = rewards + GAMMA * ((1 - done_data) * torch.max(qvals_2, dim=1)[0])\n",
    "            action_qval_pred = qvals.gather(dim=1, index=actions.long().unsqueeze(dim=1)).squeeze()\n",
    "            err = loss_fn(action_qval_pred, target.detach())\n",
    "            losses.append(err.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            err.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    print(f'EPOCH: {i}, total reward: {np.sum(epoch_rewards)}, timestep: {j}, epsilon: {EPSILON}')\n",
    "    if EPSILON > MIN_EPSILON:\n",
    "        EPSILON -= 1 / EPOCH\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "No Unity environment is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Nathan\\!SKRIPSI\\code-2\\model-qlearning.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Nathan/%21SKRIPSI/code-2/model-qlearning.ipynb#ch0000005?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.6\\lib\\site-packages\\mlagents_envs\\environment.py:412\u001b[0m, in \u001b[0;36mUnityEnvironment.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/USER/.pyenv/pyenv-win/versions/3.9.6/lib/site-packages/mlagents_envs/environment.py?line=409'>410</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close()\n\u001b[0;32m    <a href='file:///c%3A/Users/USER/.pyenv/pyenv-win/versions/3.9.6/lib/site-packages/mlagents_envs/environment.py?line=410'>411</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/USER/.pyenv/pyenv-win/versions/3.9.6/lib/site-packages/mlagents_envs/environment.py?line=411'>412</a>\u001b[0m     \u001b[39mraise\u001b[39;00m UnityEnvironmentException(\u001b[39m\"\u001b[39m\u001b[39mNo Unity environment is loaded.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WAVE environment created.\n",
      "1\n",
      "Decision Steps\n",
      "[[0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n",
      "Preds\n",
      "[[5.0858493 5.257706 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.2514], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([3.8715], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.045248657\n",
      "==================================\n",
      "2\n",
      "Decision Steps\n",
      "[[5.72632   0.8788889 1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.       ]]\n",
      "Preds\n",
      "[[3.8740757 4.251408 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.2797], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.0629], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.21112537\n",
      "==================================\n",
      "3\n",
      "Decision Steps\n",
      "[[5.2240553 1.8152071 1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.       ]]\n",
      "Preds\n",
      "[[3.9802969 4.279697 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.4073], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.3232], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.35663036\n",
      "==================================\n",
      "4\n",
      "Decision Steps\n",
      "[[4.3969693 2.598745  1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.       ]]\n",
      "Preds\n",
      "[[4.19967   4.4072976]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.6552], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.6681], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.47839314\n",
      "==================================\n",
      "5\n",
      "Decision Steps\n",
      "[[3.2965124 3.2544317 1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.       ]]\n",
      "Preds\n",
      "[[4.5333624 4.6552024]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([5.0078], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([5.0873], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.5802877\n",
      "==================================\n",
      "6\n",
      "Decision Steps\n",
      "[[1.9911051 3.8031292 1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.        1.\n",
      "  1.       ]]\n",
      "Preds\n",
      "[[4.9381256 5.007773 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([5.3409], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([5.4724], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.66555583\n",
      "==================================\n",
      "7\n",
      "Decision Steps\n",
      "[[0.56188107 4.262294   1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "Preds\n",
      "[[5.260086 5.340943]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([5.2464], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([5.4587], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.7369106\n",
      "==================================\n",
      "8\n",
      "Decision Steps\n",
      "[[-0.90225935  4.646536    1.          1.          1.          1.\n",
      "   0.          0.9963823   1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[5.0843406 5.2463827]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.9829], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([5.2812], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.7966223\n",
      "==================================\n",
      "9\n",
      "Decision Steps\n",
      "[[-2.3103118  4.9680805  1.         1.         1.         1.\n",
      "   0.         0.8988362  1.         1.         0.         0.9494696\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         1.       ]]\n",
      "Preds\n",
      "[[4.606265  4.9828734]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.5687], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.9585], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.8465907\n",
      "==================================\n",
      "10\n",
      "Decision Steps\n",
      "[[-3.574723   5.2371583  1.         1.         1.         1.\n",
      "   0.         0.8071489  1.         1.         0.         0.8568646\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         1.\n",
      "   1.         1.         1.         1.         1.         1.       ]]\n",
      "Preds\n",
      "[[3.9470758 4.5687437]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.1869], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.6566], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.88840574\n",
      "==================================\n",
      "11\n",
      "Decision Steps\n",
      "[[-4.6168985   5.4623303   1.          1.          1.          1.\n",
      "   0.          0.7158024   1.          1.          0.          0.75989175\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[3.368441  4.1868887]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([3.7213], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.2725], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.9233977\n",
      "==================================\n",
      "12\n",
      "Decision Steps\n",
      "[[-5.371985    5.650761    0.          0.60908055  1.          1.\n",
      "   0.          0.6210132   1.          1.          0.          0.6592641\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[2.7907178 3.7212508]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([3.7081], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.2899], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.9526801\n",
      "==================================\n",
      "13\n",
      "Decision Steps\n",
      "[[-5.7930546   5.8084455   0.          0.513287    0.          0.5233429\n",
      "   0.          0.5233429   1.          1.          0.          0.55557793\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[2.7162998 3.708071 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.2049], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.7616], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.9771844\n",
      "==================================\n",
      "14\n",
      "Decision Steps\n",
      "[[-5.8539877   5.9404      0.          0.4151287   0.          0.4232616\n",
      "   0.          0.4232616   0.          0.44933203  1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[3.342985 4.204946]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.4999], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([5.0476], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "0.9976904\n",
      "==================================\n",
      "15\n",
      "Decision Steps\n",
      "[[-5.550897    6.050823    0.          0.31499156  0.          0.3211627\n",
      "   1.          1.          0.          0.3409444   1.          1.\n",
      "   0.          0.37883714  1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[3.7326233 4.499871 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.2572], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.8463], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0148503\n",
      "==================================\n",
      "16\n",
      "Decision Steps\n",
      "[[-4.9027085   6.143228    1.          1.          0.          0.22170724\n",
      "   1.          1.          0.          0.23076458  1.          1.\n",
      "   0.          0.25641182  1.          1.          0.          0.30150843\n",
      "   1.          1.          0.          0.38374743  1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[3.6220853 4.2572   ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([3.9430], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([4.5779], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0292101\n",
      "==================================\n",
      "17\n",
      "Decision Steps\n",
      "[[-3.949684    6.220556    1.          1.          0.          0.9618345\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   0.          0.18854064  1.          1.          0.          0.16249841\n",
      "   1.          1.          0.          0.19803064  1.          1.\n",
      "   0.          0.28749588  1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[3.2195756 3.94299  ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.7287], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([5.2971], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0412269\n",
      "==================================\n",
      "18\n",
      "Decision Steps\n",
      "[[-2.751112    6.285266    1.          1.          0.          0.86123216\n",
      "   1.          1.          0.          0.9081431   1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   0.          0.17196056  1.          1.          0.          0.16198327\n",
      "   1.          1.          1.          1.          1.          1.        ]]\n",
      "Preds\n",
      "[[4.194292 4.728741]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([5.3496], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([5.8659], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0512829\n",
      "==================================\n",
      "19\n",
      "Decision Steps\n",
      "[[-1.3814569   6.339417    1.          1.          1.          1.\n",
      "   1.          1.          0.          0.79415786  1.          1.\n",
      "   0.          0.8824208   1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          1.          1.          1.          1.\n",
      "   1.          1.          0.          0.20071453  1.          1.        ]]\n",
      "Preds\n",
      "[[5.1689944 5.349569 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([4.5523], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([5.1568], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0596981\n",
      "==================================\n",
      "20\n",
      "Decision Steps\n",
      "[[0.07407665 6.384732   1.         1.         1.         1.\n",
      "  1.         1.         0.         0.6792936  1.         1.\n",
      "  0.         0.7547905  1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "Preds\n",
      "[[4.478644 4.552347]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([2.9325], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([3.7060], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.06674\n",
      "==================================\n",
      "21\n",
      "Decision Steps\n",
      "[[1.5250087  6.4226527  1.         1.         1.         1.\n",
      "  1.         1.         0.         0.5636936  1.         1.\n",
      "  0.         0.62634265 1.         1.         0.         0.73650116\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "Preds\n",
      "[[2.841506  2.9324658]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([2.2070], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([3.0589], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.072633\n",
      "==================================\n",
      "22\n",
      "Decision Steps\n",
      "[[2.8811216  6.454386   1.         1.         1.         1.\n",
      "  1.         1.         0.         0.44747815 1.         1.\n",
      "  0.         0.49721098 1.         1.         0.         0.5846583\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "Preds\n",
      "[[2.0715423 2.206955 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([1.6673], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([2.5782], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0775645\n",
      "==================================\n",
      "23\n",
      "Decision Steps\n",
      "[[4.058075   6.480942   1.         1.         1.         1.\n",
      "  1.         1.         0.         0.33074772 1.         1.\n",
      "  0.         0.36750716 1.         1.         0.         0.43214267\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "Preds\n",
      "[[1.4468642 1.6673255]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([-0.3651], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([0.7531], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0816913\n",
      "==================================\n",
      "24\n",
      "Decision Steps\n",
      "[[4.9827814  6.5031643  1.         1.         0.         0.20119374\n",
      "  1.         1.         0.         0.21358614 1.         1.\n",
      "  0.         0.23732418 1.         1.         0.         0.27906373\n",
      "  1.         1.         0.         0.35518074 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "Preds\n",
      "[[-0.71933305 -0.3651367 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([-1.7124], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([-0.4560], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0851445\n",
      "==================================\n",
      "25\n",
      "Decision Steps\n",
      "[[5.5975885  6.52176    0.         0.08875122 0.         0.09048997\n",
      "  0.         0.09048998 0.         0.09606364 0.         0.09606364\n",
      "  0.         0.10674017 0.         0.1068499  0.         0.12551318\n",
      "  1.         1.         0.         0.15974802 1.         1.\n",
      "  0.         0.2319182  1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "Preds\n",
      "[[-2.179752  -1.7123947]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "0\n",
      "Next Preds\n",
      "tensor([2.7805], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([3.5905], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "1.0880344\n",
      "==================================\n",
      "26\n",
      "Decision Steps\n",
      "[[5.8644686  6.537322   0.         0.813229   0.         0.8291613\n",
      "  0.         0.83063495 1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.         1.         1.         1.         1.         1.        ]]\n",
      "Preds\n",
      "[[2.4597275 2.780465 ]]\n",
      "Selected Preds\n",
      "1\n",
      "Done?\n",
      "1\n",
      "Next Preds\n",
      "tensor([0.], grad_fn=<MulBackward0>)\n",
      "Target\n",
      "tensor([-9.7821], grad_fn=<AddBackward0>)\n",
      "Reward\n",
      "-9.782089\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='./Wave', seed=1)\n",
    "print(\"WAVE environment created.\")\n",
    "\n",
    "env.reset()\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    behavior_name = list(env.behavior_specs)[0]\n",
    "\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    if (len(terminal_steps) > 0):\n",
    "        break\n",
    "\n",
    "    preds = model(torch.Tensor(preprocess_input(decision_steps))).detach().numpy()\n",
    "\n",
    "    action = np.argmax(preds)\n",
    "    action_tuple = ActionTuple()\n",
    "    action_tuple.add_discrete(np.array([[action]]))\n",
    "    env.set_actions(behavior_name, action_tuple)\n",
    "    env.step()\n",
    "\n",
    "    new_decision_steps, new_terminal_steps = env.get_steps(behavior_name)\n",
    "    current_step = new_decision_steps if len(new_terminal_steps) == 0 else new_terminal_steps\n",
    "    try:\n",
    "        reward = current_step.reward[0]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    qvals_2 = model(torch.Tensor(preprocess_input(current_step)))\n",
    "    target = reward + GAMMA * ((1 - int(len(new_terminal_steps) > 0)) * torch.max(qvals_2, dim=1)[0])\n",
    "    print(i, 'Decision Steps', preprocess_input(decision_steps), 'Preds', preds, 'Selected Preds', np.argmax(preds), 'Done?', int(len(new_terminal_steps) > 0), 'Next Preds', ((1 - int(len(new_terminal_steps) > 0)) * torch.max(qvals_2, dim=1)[0]), 'Target', target, 'Reward', reward, sep='\\n')\n",
    "    print('==================================')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnityEnvironmentException",
     "evalue": "No Unity environment is loaded.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32me:\\Nathan\\!SKRIPSI\\code-2\\model-qlearning.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Nathan/%21SKRIPSI/code-2/model-qlearning.ipynb#ch0000007?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mclose()\n",
      "File \u001b[1;32m~\\.pyenv\\pyenv-win\\versions\\3.9.6\\lib\\site-packages\\mlagents_envs\\environment.py:412\u001b[0m, in \u001b[0;36mUnityEnvironment.close\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/USER/.pyenv/pyenv-win/versions/3.9.6/lib/site-packages/mlagents_envs/environment.py?line=409'>410</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close()\n\u001b[0;32m    <a href='file:///c%3A/Users/USER/.pyenv/pyenv-win/versions/3.9.6/lib/site-packages/mlagents_envs/environment.py?line=410'>411</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/USER/.pyenv/pyenv-win/versions/3.9.6/lib/site-packages/mlagents_envs/environment.py?line=411'>412</a>\u001b[0m     \u001b[39mraise\u001b[39;00m UnityEnvironmentException(\u001b[39m\"\u001b[39m\u001b[39mNo Unity environment is loaded.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mUnityEnvironmentException\u001b[0m: No Unity environment is loaded."
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ca9c2878fbaf906546635f1e51bfc073e4e43dd47adf91e04b336b35cf7c5ff3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
